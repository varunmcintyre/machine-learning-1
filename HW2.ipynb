{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c58034f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Coefficients\n",
      "bedrooms      -2.827722e+01\n",
      "bathrooms      3.139104e+01\n",
      "sqft_living    2.843715e+15\n",
      "sqft_lot       4.534507e+00\n",
      "floors         4.927442e-01\n",
      "waterfront     5.070269e+01\n",
      "view           3.888898e+01\n",
      "condition      2.108529e+01\n",
      "grade          1.139387e+02\n",
      "sqft_above    -2.563970e+15\n",
      "sqft_basement -1.370319e+15\n",
      "yr_built      -7.289534e+01\n",
      "yr_renovated   1.094533e+01\n",
      "lat            7.784047e+01\n",
      "long          -1.578261e+01\n",
      "sqft_living15  1.753381e+01\n",
      "sqft_lot15    -9.951207e+00\n",
      "\n",
      "MSE: 40897.45\n",
      "R2: 0.696\n"
     ]
    }
   ],
   "source": [
    "# Problem 2a\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import statistics as st\n",
    "\n",
    "data = pd.read_csv(\"kc_house_data.csv\")\n",
    "\n",
    "def scale(data):\n",
    "    # standardize every feature in dataset\n",
    "    feat_scaled = data.drop(columns=[\"id\", \"date\", \"price\", \"zipcode\"])\n",
    "    feat_names = feat_scaled.columns.tolist()\n",
    "    for feat in feat_names:\n",
    "        feat_scaled[feat] = (feat_scaled[feat] - st.mean(feat_scaled[feat])) / st.stdev(feat_scaled[feat])\n",
    "\n",
    "    # scale price by dividing by 1000\n",
    "    price_scaled = data[\"price\"] / 1000\n",
    "    \n",
    "    return feat_scaled, price_scaled\n",
    "\n",
    "X_scaled, y_scaled = scale(data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=5)\n",
    "\n",
    "# create model and fit to training data\n",
    "MLR_model = LinearRegression()\n",
    "MLR_model.fit(X_train, y_train)\n",
    "y_train_pred = MLR_model.predict(X_train)\n",
    "\n",
    "# calculate coefficients, MSE, and R2\n",
    "coeffs = MLR_model.coef_\n",
    "feat_names = X_scaled.columns.tolist()\n",
    "coeffs_table = pd.DataFrame(coeffs, feat_names, columns=[\"Coefficients\"])\n",
    "\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(coeffs_table)\n",
    "print(\"\\nMSE:\",round(mse, 3))\n",
    "print(\"R2:\",round(r2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e547a28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 41747.783\n",
      "R2: 0.694\n"
     ]
    }
   ],
   "source": [
    "# Problem 2b\n",
    "\n",
    "# use trained model for test set\n",
    "y_test_pred = MLR_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_test_pred)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"MSE:\",round(mse, 3))\n",
    "print(\"R2:\",round(r2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b8ec1",
   "metadata": {},
   "source": [
    "### Problem 2c\n",
    "\n",
    "sqft_living, sqft_above, and sqft_basement all contributed the most to the linear regression model by far because they had the greatest coefficients. As far as error, the model fit the data somewhat well, with an MSE of about 40879, which is decent considering the size of the data set. The R2 value of 0.696 shows that there is a moderately strong correlation between the features and price. The MSE and R2 for the testing are almost identical to the training, which shows that the fitted model is equally as accurate with the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "ef41c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3a\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# convert train-test splits to 2d numpy arrays\n",
    "X_trn = X_train.values\n",
    "y_trn = y_train.values\n",
    "X_tst = X_test.values\n",
    "y_tst = y_test.values\n",
    "\n",
    "# add column of ones as first column\n",
    "def add_ones(x):\n",
    "    ones_col = np.ones(len(x))\n",
    "    x = np.c_[ones_col,x]\n",
    "    return x\n",
    "\n",
    "# calculate theta using closed-form solution\n",
    "def closed_solution(X, y):\n",
    "\n",
    "    XT = np.transpose(X)\n",
    "    theta = np.linalg.inv(XT @ X) @ XT @ y\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# h theta of (x) function\n",
    "def h_theta(X, theta):\n",
    "    \n",
    "    return X @ theta\n",
    "\n",
    "# add bias column to Xs\n",
    "X_trn = add_ones(X_trn)\n",
    "X_tst = add_ones(X_tst)\n",
    "\n",
    "# get theta from closed form solution\n",
    "theta = closed_solution(X_trn, y_trn)\n",
    "\n",
    "# get prediction using training data\n",
    "y_trn_predict = h_theta(X_trn, theta)\n",
    "\n",
    "# get prediction using testing data\n",
    "y_tst_predict = h_theta(X_tst, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "cb8638c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for training: 41585.425\n",
      "R2 for training: 0.69\n",
      "\n",
      "MSE for testing: 42578.37\n",
      "R2 for testing: 0.688\n"
     ]
    }
   ],
   "source": [
    "# Problem 3b\n",
    "\n",
    "train_mse = mean_squared_error(y_trn, y_trn_predict)\n",
    "test_mse = mean_squared_error(y_tst, y_tst_predict)\n",
    "train_r2 = r2_score(y_train, y_trn_predict)\n",
    "test_r2 = r2_score(y_test, y_tst_predict)\n",
    "\n",
    "print(\"MSE for training:\",round(train_mse, 3))\n",
    "print(\"R2 for training:\",round(train_r2, 3))\n",
    "\n",
    "print(\"\\nMSE for testing:\",round(test_mse, 3))\n",
    "print(\"R2 for testing:\",round(test_r2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00934555",
   "metadata": {},
   "source": [
    "The MSE and R2 for both the training and testing data is nearly identical to the corresponding MSE and R2 values from Problem 3, with an MSE of around 41,000 to 42,000 and an R2 of about 0.69. Implementing the closed-form solution worked effectively the same as the built-in linear regression package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "407fb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4a\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def poly_reg(X, Y, p):\n",
    "    \n",
    "    # make X 2D with 1 column\n",
    "    X = X.reshape(-1, 1)\n",
    "    \n",
    "    # add extra features of x up to the degree of p\n",
    "    poly = PolynomialFeatures(degree=p)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    theta = closed_solution(X_poly, Y)\n",
    "    poly_pred = h_theta(X_poly, theta)\n",
    "\n",
    "    return poly_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0c204fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Train MSE  Train R2   Test MSE  Test R2\n",
      "Deg. 1  68389.622     0.491  68191.915    0.500\n",
      "Deg. 2  62962.356     0.531  62963.083    0.539\n",
      "Deg. 3  61922.848     0.539  62866.131    0.539\n",
      "Deg. 4  61388.736     0.543  62554.140    0.541\n",
      "Deg. 5  60781.205     0.548  61523.281    0.549\n"
     ]
    }
   ],
   "source": [
    "# Problem 4b\n",
    "\n",
    "# get only the sqft_living feature for \n",
    "X_trn_sqft = X_train[\"sqft_living\"].values\n",
    "X_tst_sqft = X_test[\"sqft_living\"].values\n",
    "\n",
    "evals = np.empty((5,4))\n",
    "\n",
    "for i in range(5):  \n",
    "    # get polynomial predictions\n",
    "    y_trn_pred = poly_reg(X_trn_sqft, y_trn, i+1)\n",
    "    y_tst_pred = poly_reg(X_tst_sqft, y_tst, i+1)\n",
    "\n",
    "    # get MSEs and R2s for training and testing predictions\n",
    "    evals[i][0] = round(mean_squared_error(y_trn, y_trn_pred),3)\n",
    "    evals[i][1] = round(r2_score(y_trn, y_trn_pred),3) \n",
    "    evals[i][2] = round(mean_squared_error(y_tst, y_tst_pred),3)\n",
    "    evals[i][3] = round(r2_score(y_tst, y_tst_pred),3)\n",
    "\n",
    "cols = [\"Train MSE\",\"Train R2\",\"Test MSE\",\"Test R2\"]\n",
    "rows = [\"Deg. 1\",\"Deg. 2\",\"Deg. 3\",\"Deg. 4\",\"Deg. 5\"]\n",
    "report_table = pd.DataFrame(evals, columns=cols, index=rows)\n",
    "print(report_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8643b2",
   "metadata": {},
   "source": [
    "As the degree increases, MSE decreases consistently and R2 increases consistently. Overall the model is better fit at higher degree polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "c8292acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 5a\n",
    "\n",
    "def GD(X, y, alpha, iters):\n",
    "    \n",
    "    # initialize theta\n",
    "    theta = closed_solution(X, y)\n",
    "    \n",
    "    # gradient descent\n",
    "    for i in range(1,iters):\n",
    "        theta = theta - (alpha * (2 / len(X)) * np.transpose((X @ theta) - y) @ X)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "82cbd1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE:\n",
      "                  a=0.01         a=0.1          a=0.5\n",
      "10 iter.   41268.599049  40921.796044   9.093472e+12\n",
      "50 iter.   40970.293199  40896.557540   5.103008e+61\n",
      "100 iter.  40920.203402  40896.329246  4.407547e+122\n",
      "\n",
      "Training R2:\n",
      "              a=0.01     a=0.1          a=0.5\n",
      "10 iter.   0.692853  0.695434  -6.767943e+07\n",
      "50 iter.   0.695073  0.695622  -3.797985e+56\n",
      "100 iter.  0.695446  0.695623 -3.280378e+117\n",
      "\n",
      "Testing MSE:\n",
      "                  a=0.01         a=0.1          a=0.5\n",
      "10 iter.   42226.593136  41764.982834   9.544558e+12\n",
      "50 iter.   41835.295787  41722.043443   5.356127e+61\n",
      "100 iter.  41764.093799  41718.414828  4.626169e+122\n",
      "\n",
      "Testing R2:\n",
      "              a=0.01     a=0.1          a=0.5\n",
      "10 iter.   0.692853  0.695434  -6.767943e+07\n",
      "50 iter.   0.695073  0.695622  -3.797985e+56\n",
      "100 iter.  0.695446  0.695623 -3.280378e+117\n"
     ]
    }
   ],
   "source": [
    "# Problem 5b\n",
    "\n",
    "iters = [10,50,100]\n",
    "alphas = [0.01,0.1,0.5]\n",
    "\n",
    "trn_mse = np.empty((3,3))\n",
    "trn_r2 = np.empty((3,3))\n",
    "tst_mse = np.empty((3,3))\n",
    "tst_r2 = np.empty((3,3))\n",
    "\n",
    "# get predictions and MSE/R2 for all alphas and iterations\n",
    "for i in range(len(iters)):\n",
    "    for a in range(len(alphas)):\n",
    "        trained_GD = GD(X_trn, y_trn, alphas[a], iters[i])\n",
    "        \n",
    "        pred_trn = h_theta(X_trn, trained_GD)\n",
    "        pred_tst = h_theta(X_tst, trained_GD)\n",
    "        \n",
    "        trn_mse[i][a] = mean_squared_error(y_trn, pred_trn)\n",
    "        tst_mse[i][a] = mean_squared_error(y_tst, pred_tst)\n",
    "        trn_r2[i][a] = r2_score(y_trn, pred_trn)\n",
    "        tst_r2[i][a] = r2_score(y_tst, pred_tst)\n",
    "\n",
    "# make tables for training and testing MSE and R2\n",
    "cols = [\"a=0.01\",\"a=0.1\",\"a=0.5\"]\n",
    "rows = [\"10 iter.\",\"50 iter.\",\"100 iter.\"]\n",
    "trn_mse_table = pd.DataFrame(trn_mse, columns=cols, index=rows)\n",
    "trn_r2_table = pd.DataFrame(trn_r2, columns=cols, index=rows)\n",
    "tst_mse_table = pd.DataFrame(tst_mse, columns=cols, index=rows)\n",
    "tst_r2_table = pd.DataFrame(trn_r2, columns=cols, index=rows)\n",
    "\n",
    "# print tables\n",
    "print(\"Training MSE:\\n\",trn_mse_table)\n",
    "print(\"\\nTraining R2:\\n\",trn_r2_table)\n",
    "print(\"\\nTesting MSE:\\n\",tst_mse_table)\n",
    "print(\"\\nTesting R2:\\n\",tst_r2_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa28a8",
   "metadata": {},
   "source": [
    "### Problem 5c\n",
    "\n",
    "As both the iterations and alpha increase, the MSE decreases and R2 increases. However, at alpha = 0.5, MSE is extremely high and R2 is completely out of bounds. This is because when the step size is too large, the gradient descent may overshoot, fail to converage, or diverage, resulting in an inaccurate model. Of the given iterations and alpha levels, 100 iterations and alpha = 0.1 gave the lowest MSE and highest R2. These likely lead the model very close to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "85ad0721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89.98067069 -44.43545388 -60.1687786  -57.72635572 -18.70596645\n",
      " -44.55297394   2.66171158  -7.90475559  23.26574182 -52.31737776\n",
      " -61.32728237  -5.04700107 -56.57741253   4.61302053  15.21090766\n",
      " -39.86059206 -54.34472173 -20.69022941]\n"
     ]
    }
   ],
   "source": [
    "# Problem 6b\n",
    "\n",
    "def GD_ridge(X, y, alpha, iters, lmbda):\n",
    "    # initialize theta\n",
    "    theta = closed_solution(X, y)\n",
    "    \n",
    "    # gradient descent with ridge regression\n",
    "    for j in range(1,iters):\n",
    "        theta = theta * (1 - (alpha * lmbda)) - (alpha * (2/len(X)) * np.transpose((X @ theta) - y) @ X)\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# test that GD_ridge works with arbitrary lambda\n",
    "gd_ridge = GD_ridge(X_trn, y_trn, 0.1, 100, 10)\n",
    "print(gd_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "70932290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "slope: 2.002\n",
      "MSE: 0.063\n",
      "R2: 0.989\n",
      "\n",
      "Ridge Regression for lambda = 1\n",
      "slope: 2.0\n",
      "MSE: 0.063\n",
      "R2: 0.989\n",
      "\n",
      "Ridge Regression for lambda = 10\n",
      "slope: 1.987\n",
      "MSE: 0.063\n",
      "R2: 0.988\n",
      "\n",
      "Ridge Regression for lambda = 100\n",
      "slope: 1.865\n",
      "MSE: 0.121\n",
      "R2: 0.978\n",
      "\n",
      "Ridge Regression for lambda = 1000\n",
      "slope: 1.153\n",
      "MSE: 2.026\n",
      "R2: 0.629\n",
      "\n",
      "Ridge Regression for lambda = 10000\n",
      "slope: 0.239\n",
      "MSE: 7.564\n",
      "R2: -0.386\n"
     ]
    }
   ],
   "source": [
    "# Problem 6c\n",
    "\n",
    "# create X with random uniform distribution\n",
    "X = np.empty((1000,1))\n",
    "for i in range(1000):\n",
    "    X[i] = np.random.uniform(-2, 2)\n",
    "    \n",
    "# generate gaussian distribution with 1000 entries with mean 1 and stdev 0.25\n",
    "# points will fall between 0 and 2\n",
    "e = np.random.normal(1, 0.25, 1000)\n",
    "e = e.reshape(-1,1)\n",
    "\n",
    "# get ones for first term in Y\n",
    "ones = np.ones(1000)\n",
    "ones = ones.reshape(-1,1)\n",
    "\n",
    "Y = ones + (2*X) + e\n",
    "\n",
    "# closed form solution for ridge regression\n",
    "def closed_ridge(X, Y, lmbda):\n",
    "    XT = np.transpose(X)\n",
    "    theta = np.linalg.inv((XT @ X) + (lmbda * np.eye(2))) @ XT @ Y\n",
    "    return theta\n",
    "\n",
    "X = add_ones(X)\n",
    "\n",
    "# get linear prediction and slope/MSE/R2\n",
    "theta_linear = closed_solution(X, Y)\n",
    "pred_linear = h_theta(X, theta_linear)\n",
    "print(\"Linear Regression\")\n",
    "print(\"slope:\",round(theta_linear[1][0], 3))\n",
    "print(\"MSE:\",round(mean_squared_error(Y, pred_linear), 3))\n",
    "print(\"R2:\",round(r2_score(Y, pred_linear), 3))\n",
    "\n",
    "# get ridge prediction and slope/MSE/R2 for all levels of lambda\n",
    "for l in [1,10,100,1000,10000]:\n",
    "    theta_ridge = closed_ridge(X, Y, l)\n",
    "    pred_ridge = h_theta(X, theta_ridge)\n",
    "    print(\"\\nRidge Regression for lambda =\",l)\n",
    "    print(\"slope:\",round(theta_ridge[1][0], 3))\n",
    "    print(\"MSE:\",round(mean_squared_error(Y, pred_ridge), 3))\n",
    "    print(\"R2:\",round(r2_score(Y, pred_ridge), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e7cbd0",
   "metadata": {},
   "source": [
    "As the regularization parameter lambda increases, slope decreases, MSE increases, and R2 decreases, indicating an overall decrease in model fit. A lambda that is too high causes underfitting, because it reduces the model complexity too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830117e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
